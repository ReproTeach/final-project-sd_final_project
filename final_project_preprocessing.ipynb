{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a6a2f80a-c74a-4d4c-a0f8-8d82f0e5f7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nrclex in /opt/conda/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.10/site-packages (from nrclex) (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.10/site-packages (from textblob->nrclex) (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (4.65.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob->nrclex) (8.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nrclex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "10d51fe3-39bb-4b75-b242-61267848647c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from nrclex import NRCLex\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "80efa00f-7e29-4f75-8e71-c6b4f87031e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1cff3a28-912d-4a00-9f91-c3f47ace4cea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>44958</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>Do you remember the last time you paid $2.99 a...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35526</th>\n",
       "      <td>44946</td>\n",
       "      <td>89898</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>YÂ’all really shitting that much more at home?...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35527</th>\n",
       "      <td>44948</td>\n",
       "      <td>89900</td>\n",
       "      <td>Toronto, Ontario</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>Still shocked by the number of #Toronto superm...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35528</th>\n",
       "      <td>44949</td>\n",
       "      <td>89901</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>I never that weÂ’d be in a situation &amp;amp; wor...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35529</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35530</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35531 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location    TweetAt  \\\n",
       "0             1       44953                           NYC 2020-03-02   \n",
       "1             2       44954                   Seattle, WA 2020-03-02   \n",
       "2             4       44956                   Chicagoland 2020-03-02   \n",
       "3             5       44957           Melbourne, Victoria 2020-03-03   \n",
       "4             6       44958                   Los Angeles 2020-03-03   \n",
       "...         ...         ...                           ...        ...   \n",
       "35526     44946       89898                  Brooklyn, NY 2020-04-14   \n",
       "35527     44948       89900              Toronto, Ontario 2020-04-14   \n",
       "35528     44949       89901                          OHIO 2020-04-14   \n",
       "35529     44951       89903  Wellington City, New Zealand 2020-04-14   \n",
       "35530     44955       89907  i love you so much || he/him 2020-04-14   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \n",
       "0      TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1      When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2      #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "3      #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  \n",
       "4      Do you remember the last time you paid $2.99 a...             Neutral  \n",
       "...                                                  ...                 ...  \n",
       "35526  YÂ’all really shitting that much more at home?...            Negative  \n",
       "35527  Still shocked by the number of #Toronto superm...            Negative  \n",
       "35528  I never that weÂ’d be in a situation &amp; wor...            Positive  \n",
       "35529  Airline pilots offering to stock supermarket s...             Neutral  \n",
       "35530  @TartiiCat Well new/used Rift S are going for ...            Negative  \n",
       "\n",
       "[35531 rows x 6 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and clean dataset\n",
    "def load_and_clean_data(file_1, file_2):\n",
    "    df_1 = pd.read_csv(file_1)\n",
    "    df_1.dropna(inplace=True)\n",
    "    df_1.drop_duplicates(inplace=True)\n",
    "\n",
    "    df_2 = pd.read_csv(file_2)\n",
    "    df_2.dropna(inplace=True)\n",
    "    df_2.drop_duplicates(inplace=True)\n",
    "\n",
    "    merged_df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "    merged_df['TweetAt'] = pd.to_datetime(merged_df['TweetAt'], format='%d-%m-%Y')\n",
    "\n",
    "    return merged_df\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "08bd57b3-e132-4eca-a44b-2b01b8108853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by the month of March and April\n",
    "filtered_df = merged_df[merged_df['TweetAt'].dt.month.isin([3, 4])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d8be8d8c-ca03-47a7-82b3-2f72f5a827c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "filtered_df.loc[:, 'OriginalTweet'] = filtered_df['OriginalTweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "153a801d-473a-45de-86f8-e7daaefe3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams\n",
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "filtered_df['bigrams'] = filtered_df['OriginalTweet'].apply(lambda x: get_ngrams(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "27c4c381-fbb6-45c5-a708-4259ab1eb729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('grocery store', 4137), ('to the', 3363), ('in the', 3154), ('of the', 2633), ('’ s', 2145), ('covid 19', 2139), ('the grocery', 1923), ('the coronavirus', 1919), ('coronavirus covid19', 1705), ('due to', 1608)]\n"
     ]
    }
   ],
   "source": [
    "# Compute n-gram frequency\n",
    "counter = Counter()\n",
    "\n",
    "for bigrams in filtered_df['bigrams']:\n",
    "    counter.update(bigrams)\n",
    "\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6007de63-d96d-4989-a7c2-7760f62c2cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub('['+punctuation+']', '', text)  # Remove punctuations\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "251d531c-9223-45a3-b95e-6bc804db21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing and ngram generation to each tweet\n",
    "filtered_df['preprocessed_bigrams'] = filtered_df['OriginalTweet'].apply(lambda x: list(ngrams(preprocess(x), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f1f95fe3-4d93-403d-bb27-7cb854fdd096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grocery store 4138\n",
      "covid 19 2139\n",
      "coronavirus covid19 1715\n",
      "covid19 coronavirus 1491\n",
      "hand sanitizer 1237\n",
      "online shopping 1211\n",
      "toilet paper 1170\n",
      "panic buying 940\n",
      "â “ 896\n",
      "oil prices 811\n"
     ]
    }
   ],
   "source": [
    "# Frequency analysis\n",
    "all_bigrams = [bigram for bigrams in filtered_df['preprocessed_bigrams'] for bigram in bigrams]\n",
    "freq_dist = FreqDist(all_bigrams)\n",
    "\n",
    "for bigram, frequency in freq_dist.most_common(10):\n",
    "    print(' '.join(bigram), frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865612b4-2165-4b3c-8ccd-1c9c463786aa",
   "metadata": {},
   "source": [
    "#### It appears that the encoding used in the dataset has caused some unusual characters to appear. We can modify our preprocess function to remove these characters and also to handle different spellings or formats of the same term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "14ce6ea4-649b-45b2-8668-b7815942bddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covid related bigrams 36002\n",
      "grocery store 4208\n",
      "hand sanitizer 1264\n",
      "online shopping 1229\n",
      "toilet paper 1198\n",
      "panic buying 1032\n",
      "oil prices 816\n",
      "coronavirus pandemic 659\n",
      "social distancing 517\n",
      "stay home 435\n",
      "stock food 419\n"
     ]
    }
   ],
   "source": [
    "# Frequency counter for 'covid' related bigrams\n",
    "covid_related_bigrams_freq = 0\n",
    "\n",
    "# Frequency counter for other bigrams\n",
    "other_bigram_freq = nltk.FreqDist()\n",
    "\n",
    "# Check each bigram\n",
    "for bigram, freq in bigram_freq.items():\n",
    "    if 'covid' in bigram[0].lower() or 'covid' in bigram[1].lower():\n",
    "        covid_related_bigrams_freq += freq\n",
    "    else:\n",
    "        other_bigram_freq[bigram] = freq\n",
    "\n",
    "# Print the frequencies\n",
    "print(f\"covid related bigrams {covid_related_bigrams_freq}\")\n",
    "\n",
    "# Print the 10 most common non-covid related bigrams\n",
    "for bigram, freq in other_bigram_freq.most_common(10):\n",
    "    print(f\"{bigram[0]} {bigram[1]} {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7794dd06-fef4-4ad4-a827-867d79b12d76",
   "metadata": {},
   "source": [
    "## preprocessing it again yay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9d8d0-d5b6-4294-aa34-0b93c164fd5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### generate bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0762da71-01d3-4126-949f-01e64f39b96d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "def get_emotion_from_text(text):\n",
    "    emotion = NRCLex(text)\n",
    "    return emotion.top_emotions[0][0]\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub('['+punctuation+']', '', text)  # Remove punctuations\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing and ngram generation to each tweet\n",
    "filtered_df['preprocessed_bigrams'] = filtered_df['OriginalTweet'].apply(lambda x: list(ngrams(preprocess(x), 2)))\n",
    "\n",
    "# Add emotion detection to the dataframe\n",
    "filtered_df['Emotion'] = filtered_df['OriginalTweet'].apply(get_emotion_from_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bf9556d0-a637-4744-b862-c3bf0b53844d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>preprocessed_bigrams</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[trending new, new yorkers, yorkers encounter,...</td>\n",
       "      <td>[(trending, new), (new, yorkers), (yorkers, en...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>when i couldnt find hand sanitizer at fred mey...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[when i, i couldnt, couldnt find, find hand, h...</td>\n",
       "      <td>[(couldnt, find), (find, hand), (hand, sanitiz...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>panic buying hits newyork city as anxious shop...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[panic buying, buying hits, hits newyork, newy...</td>\n",
       "      <td>[(panic, buying), (buying, hits), (hits, newyo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[toiletpaper dunnypaper, dunnypaper coronaviru...</td>\n",
       "      <td>[(toiletpaper, dunnypaper), (dunnypaper, coron...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>44958</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>do you remember the last time you paid 299 a g...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[do you, you remember, remember the, the last,...</td>\n",
       "      <td>[(remember, last), (last, time), (time, paid),...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35526</th>\n",
       "      <td>44946</td>\n",
       "      <td>89898</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>yâ’all really shitting that much more at home ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[yâ ’, ’ all, all really, really shitting, shi...</td>\n",
       "      <td>[(yâ, ’), (’, really), (really, shitting), (sh...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35527</th>\n",
       "      <td>44948</td>\n",
       "      <td>89900</td>\n",
       "      <td>Toronto, Ontario</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>still shocked by the number of toronto superma...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[still shocked, shocked by, by the, the number...</td>\n",
       "      <td>[(still, shocked), (shocked, number), (number,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35528</th>\n",
       "      <td>44949</td>\n",
       "      <td>89901</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>i never that weâ’d be in a situation amp world...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[i never, never that, that weâ, weâ ’, ’ d, d ...</td>\n",
       "      <td>[(never, weâ), (weâ, ’), (’, situation), (situ...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35529</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[airline pilots, pilots offering, offering to,...</td>\n",
       "      <td>[(airline, pilots), (pilots, offering), (offer...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35530</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>tartiicat well newused rift s are going for 70...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[tartiicat well, well newused, newused rift, r...</td>\n",
       "      <td>[(tartiicat, well), (well, newused), (newused,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35531 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location    TweetAt  \\\n",
       "0             1       44953                           NYC 2020-03-02   \n",
       "1             2       44954                   Seattle, WA 2020-03-02   \n",
       "2             4       44956                   Chicagoland 2020-03-02   \n",
       "3             5       44957           Melbourne, Victoria 2020-03-03   \n",
       "4             6       44958                   Los Angeles 2020-03-03   \n",
       "...         ...         ...                           ...        ...   \n",
       "35526     44946       89898                  Brooklyn, NY 2020-04-14   \n",
       "35527     44948       89900              Toronto, Ontario 2020-04-14   \n",
       "35528     44949       89901                          OHIO 2020-04-14   \n",
       "35529     44951       89903  Wellington City, New Zealand 2020-04-14   \n",
       "35530     44955       89907  i love you so much || he/him 2020-04-14   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      trending new yorkers encounter empty supermark...  Extremely Negative   \n",
       "1      when i couldnt find hand sanitizer at fred mey...            Positive   \n",
       "2      panic buying hits newyork city as anxious shop...            Negative   \n",
       "3      toiletpaper dunnypaper coronavirus coronavirus...             Neutral   \n",
       "4      do you remember the last time you paid 299 a g...             Neutral   \n",
       "...                                                  ...                 ...   \n",
       "35526  yâ’all really shitting that much more at home ...            Negative   \n",
       "35527  still shocked by the number of toronto superma...            Negative   \n",
       "35528  i never that weâ’d be in a situation amp world...            Positive   \n",
       "35529  airline pilots offering to stock supermarket s...             Neutral   \n",
       "35530  tartiicat well newused rift s are going for 70...            Negative   \n",
       "\n",
       "                                                 bigrams  \\\n",
       "0      [trending new, new yorkers, yorkers encounter,...   \n",
       "1      [when i, i couldnt, couldnt find, find hand, h...   \n",
       "2      [panic buying, buying hits, hits newyork, newy...   \n",
       "3      [toiletpaper dunnypaper, dunnypaper coronaviru...   \n",
       "4      [do you, you remember, remember the, the last,...   \n",
       "...                                                  ...   \n",
       "35526  [yâ ’, ’ all, all really, really shitting, shi...   \n",
       "35527  [still shocked, shocked by, by the, the number...   \n",
       "35528  [i never, never that, that weâ, weâ ’, ’ d, d ...   \n",
       "35529  [airline pilots, pilots offering, offering to,...   \n",
       "35530  [tartiicat well, well newused, newused rift, r...   \n",
       "\n",
       "                                    preprocessed_bigrams       Emotion  \n",
       "0      [(trending, new), (new, yorkers), (yorkers, en...          fear  \n",
       "1      [(couldnt, find), (find, hand), (hand, sanitiz...          fear  \n",
       "2      [(panic, buying), (buying, hits), (hits, newyo...      positive  \n",
       "3      [(toiletpaper, dunnypaper), (dunnypaper, coron...      positive  \n",
       "4      [(remember, last), (last, time), (time, paid),...  anticipation  \n",
       "...                                                  ...           ...  \n",
       "35526  [(yâ, ’), (’, really), (really, shitting), (sh...          fear  \n",
       "35527  [(still, shocked), (shocked, number), (number,...      positive  \n",
       "35528  [(never, weâ), (weâ, ’), (’, situation), (situ...  anticipation  \n",
       "35529  [(airline, pilots), (pilots, offering), (offer...         trust  \n",
       "35530  [(tartiicat, well), (well, newused), (newused,...      negative  \n",
       "\n",
       "[35531 rows x 9 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first few rows of the dataframe\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4526cb21-4a1e-438f-9f6a-2594cb3a58bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>preprocessed_bigrams</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[trending new, new yorkers, yorkers encounter,...</td>\n",
       "      <td>[(trending, new), (new, yorkers), (yorkers, en...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>couldnt find hand sanitizer fred meyer turned ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[when i, i couldnt, couldnt find, find hand, h...</td>\n",
       "      <td>[(couldnt, find), (find, hand), (hand, sanitiz...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>panic buying hits newyork city anxious shopper...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[panic buying, buying hits, hits newyork, newy...</td>\n",
       "      <td>[(panic, buying), (buying, hits), (hits, newyo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[toiletpaper dunnypaper, dunnypaper coronaviru...</td>\n",
       "      <td>[(toiletpaper, dunnypaper), (dunnypaper, coron...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>44958</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>remember last time paid 299 gallon regular gas...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[do you, you remember, remember the, the last,...</td>\n",
       "      <td>[(remember, last), (last, time), (time, paid),...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35526</th>\n",
       "      <td>44946</td>\n",
       "      <td>89898</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>really shitting much home covid-19 toiletpaper</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[yâ ’, ’ all, all really, really shitting, shi...</td>\n",
       "      <td>[(yâ, ’), (’, really), (really, shitting), (sh...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35527</th>\n",
       "      <td>44948</td>\n",
       "      <td>89900</td>\n",
       "      <td>Toronto, Ontario</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>still shocked number toronto supermarket emplo...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[still shocked, shocked by, by the, the number...</td>\n",
       "      <td>[(still, shocked), (shocked, number), (number,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35528</th>\n",
       "      <td>44949</td>\n",
       "      <td>89901</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>never situation amp world going supermarket pi...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[i never, never that, that weâ, weâ ’, ’ d, d ...</td>\n",
       "      <td>[(never, weâ), (weâ, ’), (’, situation), (situ...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35529</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>airline pilots offering stock supermarket shel...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[airline pilots, pilots offering, offering to,...</td>\n",
       "      <td>[(airline, pilots), (pilots, offering), (offer...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35530</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>tartiicat well newused rift going 70000 amazon...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[tartiicat well, well newused, newused rift, r...</td>\n",
       "      <td>[(tartiicat, well), (well, newused), (newused,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35531 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location    TweetAt  \\\n",
       "0             1       44953                           NYC 2020-03-02   \n",
       "1             2       44954                   Seattle, WA 2020-03-02   \n",
       "2             4       44956                   Chicagoland 2020-03-02   \n",
       "3             5       44957           Melbourne, Victoria 2020-03-03   \n",
       "4             6       44958                   Los Angeles 2020-03-03   \n",
       "...         ...         ...                           ...        ...   \n",
       "35526     44946       89898                  Brooklyn, NY 2020-04-14   \n",
       "35527     44948       89900              Toronto, Ontario 2020-04-14   \n",
       "35528     44949       89901                          OHIO 2020-04-14   \n",
       "35529     44951       89903  Wellington City, New Zealand 2020-04-14   \n",
       "35530     44955       89907  i love you so much || he/him 2020-04-14   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      trending new yorkers encounter empty supermark...  Extremely Negative   \n",
       "1      couldnt find hand sanitizer fred meyer turned ...            Positive   \n",
       "2      panic buying hits newyork city anxious shopper...            Negative   \n",
       "3      toiletpaper dunnypaper coronavirus coronavirus...             Neutral   \n",
       "4      remember last time paid 299 gallon regular gas...             Neutral   \n",
       "...                                                  ...                 ...   \n",
       "35526     really shitting much home covid-19 toiletpaper            Negative   \n",
       "35527  still shocked number toronto supermarket emplo...            Negative   \n",
       "35528  never situation amp world going supermarket pi...            Positive   \n",
       "35529  airline pilots offering stock supermarket shel...             Neutral   \n",
       "35530  tartiicat well newused rift going 70000 amazon...            Negative   \n",
       "\n",
       "                                                 bigrams  \\\n",
       "0      [trending new, new yorkers, yorkers encounter,...   \n",
       "1      [when i, i couldnt, couldnt find, find hand, h...   \n",
       "2      [panic buying, buying hits, hits newyork, newy...   \n",
       "3      [toiletpaper dunnypaper, dunnypaper coronaviru...   \n",
       "4      [do you, you remember, remember the, the last,...   \n",
       "...                                                  ...   \n",
       "35526  [yâ ’, ’ all, all really, really shitting, shi...   \n",
       "35527  [still shocked, shocked by, by the, the number...   \n",
       "35528  [i never, never that, that weâ, weâ ’, ’ d, d ...   \n",
       "35529  [airline pilots, pilots offering, offering to,...   \n",
       "35530  [tartiicat well, well newused, newused rift, r...   \n",
       "\n",
       "                                    preprocessed_bigrams       Emotion  \n",
       "0      [(trending, new), (new, yorkers), (yorkers, en...          fear  \n",
       "1      [(couldnt, find), (find, hand), (hand, sanitiz...          fear  \n",
       "2      [(panic, buying), (buying, hits), (hits, newyo...      positive  \n",
       "3      [(toiletpaper, dunnypaper), (dunnypaper, coron...      positive  \n",
       "4      [(remember, last), (last, time), (time, paid),...  anticipation  \n",
       "...                                                  ...           ...  \n",
       "35526  [(yâ, ’), (’, really), (really, shitting), (sh...          fear  \n",
       "35527  [(still, shocked), (shocked, number), (number,...      positive  \n",
       "35528  [(never, weâ), (weâ, ’), (’, situation), (situ...  anticipation  \n",
       "35529  [(airline, pilots), (pilots, offering), (offer...         trust  \n",
       "35530  [(tartiicat, well), (well, newused), (newused,...      negative  \n",
       "\n",
       "[35531 rows x 9 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra spaces\n",
    "    text = text.replace(\"covid19\", \"covid-19\")  # unify term\n",
    "    text = text.replace(\"coronavirus covid-19\", \"covid-19\")  # unify term\n",
    "    text = text.replace(\"covid-19 coronavirus\", \"covid-19\")  # unify term\n",
    "    text = text.replace(\"â\", \"\")  # remove unusual characters\n",
    "    text = text.replace(\"iâ\", \"\")  # remove unusual characters\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # remove stop words\n",
    "    return ' '.join(words)\n",
    "\n",
    "filtered_df['OriginalTweet'] = filtered_df['OriginalTweet'].apply(preprocess_text)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac9280-9c09-483a-aeb8-d26bb87376c0",
   "metadata": {},
   "source": [
    "# Cleaning the texts by creating a new collumn called \"cleaned_text\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "318cd4bb-c957-41b8-a831-16c28657639a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from nrclex import NRCLex\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6ee6d99f-397d-4c58-8ca9-045a731cac06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and clean dataset\n",
    "def load_and_clean_data(file_1, file_2):\n",
    "    df_1 = pd.read_csv(file_1)\n",
    "    df_1.dropna(inplace=True)\n",
    "    df_1.drop_duplicates(inplace=True)\n",
    "\n",
    "    df_2 = pd.read_csv(file_2)\n",
    "    df_2.dropna(inplace=True)\n",
    "    df_2.drop_duplicates(inplace=True)\n",
    "\n",
    "    merged_df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "    merged_df['TweetAt'] = pd.to_datetime(merged_df['TweetAt'], format='%d-%m-%Y')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "merged_df = load_and_clean_data('Corona_NLP_test2.csv', 'Corona_NLP_train.csv')\n",
    "\n",
    "# Filter data by the month of March and April\n",
    "filtered_df = merged_df[merged_df['TweetAt'].dt.month.isin([3, 4])].copy()\n",
    "\n",
    "# Text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "filtered_df.loc[:, 'OriginalTweet'] = filtered_df['OriginalTweet'].apply(clean_text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove RT (retweet sign)\n",
    "    text = re.sub(r'rt[\\s]+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    # Remove all non-alphabetic characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # Remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    # Remove stopwords\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "filtered_df['Text_Cleaned'] = filtered_df['OriginalTweet'].apply(preprocess_text)\n",
    "\n",
    "# Function to detect emotion\n",
    "def get_emotion(text):\n",
    "    emotion = NRCLex(text)\n",
    "    freq_dict = emotion.affect_frequencies\n",
    "    return max(freq_dict, key=freq_dict.get)\n",
    "\n",
    "# Add emotion to dataframe\n",
    "filtered_df['Emotion'] = filtered_df['Text_Cleaned'].apply(get_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "59b8f0a3-c0d1-497f-8128-f0a3e6d78fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>when i couldnt find hand sanitizer at fred mey...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>couldnt find hand sanitizer fred meyer turned ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>panic buying hits newyork city as anxious shop...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>panic buying hits newyork city anxious shopper...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>44958</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>do you remember the last time you paid 299 a g...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>remember last time paid 299 gallon regular gas...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35526</th>\n",
       "      <td>44946</td>\n",
       "      <td>89898</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>yâ’all really shitting that much more at home ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>yâ really shitting much home covid19 coronavir...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35527</th>\n",
       "      <td>44948</td>\n",
       "      <td>89900</td>\n",
       "      <td>Toronto, Ontario</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>still shocked by the number of toronto superma...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>still shocked number toronto supermarket emplo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35528</th>\n",
       "      <td>44949</td>\n",
       "      <td>89901</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>i never that weâ’d be in a situation amp world...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>never weâ situation amp world going supermarke...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35529</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>airline pilots offering stock supermarket shel...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35530</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>tartiicat well newused rift s are going for 70...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>tartiicat well newused rift going 70000 amazon...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35531 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location    TweetAt  \\\n",
       "0             1       44953                           NYC 2020-03-02   \n",
       "1             2       44954                   Seattle, WA 2020-03-02   \n",
       "2             4       44956                   Chicagoland 2020-03-02   \n",
       "3             5       44957           Melbourne, Victoria 2020-03-03   \n",
       "4             6       44958                   Los Angeles 2020-03-03   \n",
       "...         ...         ...                           ...        ...   \n",
       "35526     44946       89898                  Brooklyn, NY 2020-04-14   \n",
       "35527     44948       89900              Toronto, Ontario 2020-04-14   \n",
       "35528     44949       89901                          OHIO 2020-04-14   \n",
       "35529     44951       89903  Wellington City, New Zealand 2020-04-14   \n",
       "35530     44955       89907  i love you so much || he/him 2020-04-14   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      trending new yorkers encounter empty supermark...  Extremely Negative   \n",
       "1      when i couldnt find hand sanitizer at fred mey...            Positive   \n",
       "2      panic buying hits newyork city as anxious shop...            Negative   \n",
       "3      toiletpaper dunnypaper coronavirus coronavirus...             Neutral   \n",
       "4      do you remember the last time you paid 299 a g...             Neutral   \n",
       "...                                                  ...                 ...   \n",
       "35526  yâ’all really shitting that much more at home ...            Negative   \n",
       "35527  still shocked by the number of toronto superma...            Negative   \n",
       "35528  i never that weâ’d be in a situation amp world...            Positive   \n",
       "35529  airline pilots offering to stock supermarket s...             Neutral   \n",
       "35530  tartiicat well newused rift s are going for 70...            Negative   \n",
       "\n",
       "                                            Text_Cleaned       Emotion  \n",
       "0      trending new yorkers encounter empty supermark...          fear  \n",
       "1      couldnt find hand sanitizer fred meyer turned ...          fear  \n",
       "2      panic buying hits newyork city anxious shopper...      positive  \n",
       "3      toiletpaper dunnypaper coronavirus coronavirus...      positive  \n",
       "4      remember last time paid 299 gallon regular gas...  anticipation  \n",
       "...                                                  ...           ...  \n",
       "35526  yâ really shitting much home covid19 coronavir...          fear  \n",
       "35527  still shocked number toronto supermarket emplo...      positive  \n",
       "35528  never weâ situation amp world going supermarke...  anticipation  \n",
       "35529  airline pilots offering stock supermarket shel...         trust  \n",
       "35530  tartiicat well newused rift going 70000 amazon...      negative  \n",
       "\n",
       "[35531 rows x 8 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d975826-b8eb-41bb-b881-0644c983d443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
